{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, \n",
    "-  create a python venv, and `pip install -r requirements.txt`.\n",
    "-  make a copy of `local.env` to `.env`, configure your environment in `.env`.\n",
    "-  adjust data in the `data` folder based on your configuration.\n",
    "-  run each scripts in the `ingest` folder to process and ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "from openai.types.chat import ChatCompletionMessage\n",
    "\n",
    "sys.path.append('./utils')\n",
    "from loadenv import LLMConfig, RobotConfig\n",
    "sys.path.append('./chat/functions')\n",
    "from robot_general import get_robots, search_robot_manuals\n",
    "\n",
    "aoai_config = LLMConfig()\n",
    "aoai_api_version, aoai_deployment = aoai_config.chat[\"api_version\"], aoai_config.chat['deployment']\n",
    "\n",
    "def print_llm_messages(msg):\n",
    "  print(\"-----------------------------------LLM message(s)---------------------------------------------------\")\n",
    "  pp = pprint.PrettyPrinter(width=160)\n",
    "  if isinstance(msg, list):\n",
    "    for m in msg:\n",
    "      if isinstance(m, dict):\n",
    "        pp.pprint(m)\n",
    "      elif isinstance(m, ChatCompletionMessage):\n",
    "        pp.pprint(dict(m))\n",
    "  if isinstance(msg, ChatCompletionMessage):\n",
    "    pp.pprint(dict(msg))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=get_robots(\"petoi_cat\")\n",
    "pprint.pprint(json.loads(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=search_robot_manuals(\"api for petoi cat\")\n",
    "pprint.pprint(json.loads(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \n",
    "    \"\"\"You are an expert who helps people troubleshoot robot related issues.\n",
    "    A robot's name is typically prefixed with its kind, for example, the kind for spot_jr is spot.\n",
    "    Try identify the kind of the robot first so you can narrow down when searching for info in tools. \n",
    "    Respond only with the info from the provided context or tools.\n",
    "    If you don't know the answer, respond with \"I don't know\".\"\"\"}\n",
    "user_message = {\"role\": \"user\", \"content\": \n",
    "    \"\"\"My robot spot_jr can't make left turns. When was it purchased and last serviced? What is the API that I can call?\"\"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_robots\",\n",
    "      \"description\": \"Get the name, kind, purchase date, and manufacturer of currently registered robots\",\n",
    "      \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "              \"name\": {\"type\": \"string\", \"description\": \"The name of the robot to get information about\"},\n",
    "              \"kind\": {\"type\": \"string\", \"description\": \"the kind of the robot to get information about\"},\n",
    "          },\n",
    "          \"required\": [],\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"search_robot_manuals\",\n",
    "      \"description\": \"Search the robot manuals for robot faults, troubleshooting guides, development API etc.\",\n",
    "      \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "              \"query\": {\"type\": \"string\", \"description\": \"The user's query for semantic and vector search\"},\n",
    "              \"kind\": {\"type\": \"string\", \"description\": \"the kind of the robot to get information about\"},\n",
    "          },\n",
    "          \"required\": [\"query\"],\n",
    "      },\n",
    "    },\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "aoai = AzureOpenAI(api_version=aoai_api_version)\n",
    "messages = [system_message, user_message]\n",
    "\n",
    "num_turns = 0\n",
    "while True:\n",
    "  print_llm_messages(messages)\n",
    "  response = aoai.chat.completions.create(\n",
    "      model=aoai_deployment,\n",
    "      messages=messages,\n",
    "      tools=tools,\n",
    "      tool_choice=\"auto\", # let the model decide. \"none\" means don't call, or specify which tool to always call. \n",
    "  )\n",
    "  result_message = response.choices[0].message\n",
    "  tool_calls = result_message.tool_calls\n",
    "\n",
    "  # no more tools to call\n",
    "  if not tool_calls:\n",
    "    print_llm_messages(result_message)\n",
    "    break\n",
    "\n",
    "  # doesn't make sense to call tools over and over\n",
    "  if num_turns > 2:\n",
    "    logging.warning(\"force terminated.\")\n",
    "    print_llm_messages(result_message)\n",
    "    break\n",
    "\n",
    "  # call tools, add tool results to messages\n",
    "  result_message.content = \"\" if not result_message.content else result_message.content\n",
    "  messages.append(result_message)\n",
    "  for tool_call in tool_calls:\n",
    "    if tool_call.function.name == \"get_robots\":\n",
    "      args = json.loads(tool_call.function.arguments)\n",
    "      function_result = get_robots(**args)\n",
    "      messages.append({\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_call.function.name,\n",
    "        \"content\": function_result\n",
    "      })\n",
    "      num_turns += 1\n",
    "    elif tool_call.function.name == \"search_robot_manuals\":\n",
    "      args = json.loads(tool_call.function.arguments)\n",
    "      function_result = search_robot_manuals(**args)\n",
    "      messages.append({\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_call.function.name,\n",
    "        \"content\": function_result\n",
    "      \n",
    "      })\n",
    "      num_turns += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debuggability\n",
    "\n",
    "|Options|Pros|Cons|\n",
    "|:-----|:---|:---|\n",
    "|Python logging|<ul><li>With logging level set to INFO, http requests/responses are logged with headers.</li></ul>|<ul><li>Prompts are not logged automatically.</li><li>Format is hard to read.</li></ul>|\n",
    "|Azure Monitor|<ul><li>Out of the box dashboard with metrics for tokens, requests etc.</li> <li>With diagnostics settings enabled on Azure OpenAI, http requests/responses are logged with headers.</li></ul>|<ul><li>Prompts are not automatically logged.</li></ul>|\n",
    "|PPrint|<ul><li>Prompts can be manually logged in easy to read format.</li></ul>|<ul><li>Developer toil.</li></ul>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt flow with Azure OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in code\n",
    "\n",
    "-  Must write [flow.dag.yaml](./chat_flow/flow.dag.yaml).\n",
    "-  Certain variables, such as [Azure OpenAI deployment name](./chat_flow/flow.dag.yaml#L24), cannot be loaded from .env unless you write your own code.\n",
    "-  prompt flow tools must be annotated in certain ways. They are not the same as `tools` in OpenAI API. For example, here's a [prompt flow tool](./chat_flow/llm_tools.py)  that defines OpenAI `tools`.\n",
    "-  Flow must run from the flow folder, which makes [importing existing Python files in other folders complicated](./chat_flow/agent_tool.py).\n",
    "-  If you want to use `connections` in your non-prompt-flow code, you need to [translate](./chat_flow/agent_tool.py#L16) prompt flow `AzureOpenAIConnection` to openai `AzureOpenAI`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in debuggability\n",
    "\n",
    "-  With VSCode prompt flow extension, you can see the inputs/outputs of LLM calls automatically in the `prompt flow tab` without having to create any flow run.\n",
    "-  You can also use `pf run create -f run.yaml --variant \"${node.variant}\"` to create a run for each variant, and then use `pf run visualize --names \"run1,run2...\"` to visualize multiple runs and compare their output in a nice table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools import  Tool, tool\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search_robot_manuals\")\n",
    "def search_robot_manuals_tool(query: str, kind: str = None) -> str:\n",
    "  \"\"\"Search the robot manuals for robot faults, troubleshooting guides, development API etc.\"\"\"\n",
    "  return search_robot_manuals(query, kind)\n",
    "\n",
    "@tool(\"get_robots\")\n",
    "def get_robots_tool(name: str = None, kind: str = None) -> str:\n",
    "  \"\"\"Get info such as the name, kind, purchase date, and manufacturer of currently registered robots.\"\"\"\n",
    "  return get_robots(name, kind)\n",
    "\n",
    "lc_tools = [search_robot_manuals_tool, get_robots_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community and Composability\n",
    "\n",
    "-  With the community OpenAPI agent, you can make LLM call any API with an OpenAPI spec, not just GET, but also any operations.\n",
    "-  LangChain Agent is a model (an LLM). AgentExecutor is a chain(RunnableSerializable). So to use an existing agent as a tool, you can pass AgentExecutor.run method. Only the `run` method works, `invoke` doesn't. But `run` is deprecated.\n",
    "-  There a bigger question of what is the right agent approach? When the agents perform completely different tasks in different domains, research from Microsoft AutoGen shows that having each agent specializing on a specific domain renders better results, even though the underlying model is the same.\n",
    "    -  one agent with multiple tools?\n",
    "    -  one orchestrator agent with multiple expert agents? \n",
    "    -  group of agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits.openapi.spec import reduce_openapi_spec\n",
    "from langchain.agents.agent_toolkits.openapi import planner\n",
    "from langchain.requests import RequestsWrapper\n",
    "import yaml\n",
    "\n",
    "robotConfig = RobotConfig()\n",
    "for kind in robotConfig.kinds:\n",
    "  with open(kind['api_spec']) as f:\n",
    "    raw_api_spec = yaml.load(f, Loader=yaml.Loader)\n",
    "  api_spec = reduce_openapi_spec(raw_api_spec)\n",
    "  requests_wrapper = RequestsWrapper()\n",
    "  agent_executor: AgentExecutor = planner.create_openapi_agent(api_spec, requests_wrapper, lc_aoai)\n",
    "  # AgentExecutor is a chain\n",
    "  agent_tool = Tool.from_function(\n",
    "    func=agent_executor.run,\n",
    "    name=f\"get_{kind['name']}_maintenance_records\",\n",
    "    description=f\"Tool for accessing maintenance records for {kind['name']} robots.\")\n",
    "  lc_tools.append(agent_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start OpenAPI functions that expose maintenance data:\n",
    "\n",
    "```sh\n",
    "flask --app chat/functions/petoi.py run --port 3000\n",
    "flask --app chat/functions/spot.py run --port 5000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_aoai = AzureChatOpenAI(azure_deployment=aoai_deployment, openai_api_version=aoai_api_version)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  SystemMessage(content=system_message['content']),\n",
    "  HumanMessage(content=user_message['content']),\n",
    "  MessagesPlaceholder(variable_name=\"agent_scratchpad\")])\n",
    "\n",
    "agent = create_openai_tools_agent(lc_aoai, lc_tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=lc_tools, verbose=True)\n",
    "result = agent_executor({})\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in code\n",
    "\n",
    "-  Must use `langchain.chat_models.AzureChatOpenAI`, although unlike `promptflow.connections.AzureOpenAIConnection`, LangChain does use the same OpenAI environment variables.\n",
    "-  Must use LangChain `HumanMessage`, `SystemMessage` data types, and annotations such as `@tool`.\n",
    "-  LangChain simplifies the code by automatically calling the tools, feed tools output back to LLM.\n",
    "    -  note that it's still calling the tools one by one, it is parallizable, just not implemented that way yet.\n",
    "-  good visibility with verbose=True, excellent in langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in debuggability\n",
    "\n",
    "-  With LangSmith enabled by setting a few environment variables, all traces, including intermediate steps with inputs and outputs to LLMs, can be visualized in a nice UI.\n",
    "-  Without LangSmith, set `verbose=True` in the `AgentExecutor` also enables easy-to-visualize logging in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Assistants API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not yet available in Azure OpenAI. Assistants will automatically call built-in tools such as Code Interpreter and Knowledge Retrieval, but will wait for the developer to make Function Calling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain353py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
